{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0BWJKay9yMg3jp7RBZ2Zn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abduljunaid02/LLMs/blob/main/Makemore_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fERscQXJ3gMk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NInx5Q5-3jb-",
        "outputId": "f5d1ff3b-b6ff-4e63-f1f6-a9985e94b48a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "print(itos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NnwVEhE3yWE",
        "outputId": "21b6f2ab-33b2-41d3-fd82-4370dddd8215"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "X, Y = [], []\n",
        "for w in words:\n",
        "\n",
        "  #print(w)\n",
        "  context = [0] * block_size\n",
        "  for ch in w + '.':\n",
        "    ix = stoi[ch]\n",
        "    X.append(context)\n",
        "    Y.append(ix)\n",
        "    #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
        "    context = context[1:] + [ix] # crop and append\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)"
      ],
      "metadata": {
        "id": "otNf8qeJ3_NH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  X.shape, Y.shape, X.dtype, Y.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0fXFkNk5vDw",
        "outputId": "c875e8f3-e5ad-4c8a-92c4-eac55d0e2c68"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([228146, 3]), torch.Size([228146]), torch.int64, torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn(27,2)\n",
        "# C is our way of representing the 27 characters in 2 dimensional space.\n",
        "#We will allocate the 27 characters to these 27-2 dimenstional embeddings now"
      ],
      "metadata": {
        "id": "HqeYKPGW8roL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Direct allocation like C[5] is same as doing one hot encoding for 5 and then multiplying it with 5th entry in C\n",
        "#F.one_hot(torch.tensor(5),num_classes=27).float() @ C\n",
        "\n",
        "C[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJEiUkmp82iu",
        "outputId": "ad061e54-3f56-4779-8d39-3be246ec4aee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.4749,  0.9151])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accessing works for multiple characters as well\n",
        "\n",
        "C[torch.tensor([5,6,7])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jQlZBpy9cE-",
        "outputId": "0c2ffae0-8168-4a67-a390-5b00629d5b25"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.4749,  0.9151],\n",
              "        [-0.2999,  0.6962],\n",
              "        [ 0.5007,  0.2831]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "C[X].shape\n",
        "\n",
        "#X is our dataset. We can embedd the whole of X in C with this C[X] it returns a (32,3) i.e., of X and a 2 dimension embedding of C\n",
        "#If C were a 5 dimensional embedding it would have been (32,3,5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOIkit7b9zU-",
        "outputId": "725f80d6-01b5-437e-c653-e8e4be3eca73"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb = C[X]\n",
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNmN3EnX-FwW",
        "outputId": "4a90b036-446a-4afd-d100-d1423f08b239"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create weights and biases\n",
        "\n",
        "w1 = torch.rand(6, 100) #6 because we are using 3 characters represented in 2 dimensions to predict next character. 3*2 = 6 and 100 neurons\n",
        "b1 = torch.rand(100)"
      ],
      "metadata": {
        "id": "oJFDR3Ph_GI_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It is difficult to multiply a (32,3,2) i.e., our embedding vector with our weights that are (6,100). Matrix mul not possible\n",
        "#So convert emb to (32,6)\n",
        "\n",
        "h = torch.tanh(emb.view(-1,6) @ w1 + b1) # using -1 lets torch infer what must come in its place\n",
        "h.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l15z9RJo_z6C",
        "outputId": "2cba2a18-51a4-4518-a522-ec7486f9d52d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating w2 and b2\n",
        "\n",
        "w2 = torch.randn(100,27) # (100,27) because 100 comes from prev layers and output is 27 characters\n",
        "b2 = torch.rand(27)"
      ],
      "metadata": {
        "id": "X3cePQr2BQ-s"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = h @ w2 + b2"
      ],
      "metadata": {
        "id": "LwmrnmNICKMI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYhpYCmKCTAn",
        "outputId": "bb8fd005-dbb5-4933-f390-867fbb5324aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We have 27 outputs and we make it into the probabilities\n",
        "counts = logits.exp()\n",
        "prob = counts / counts.sum(dim=1, keepdims=True)\n",
        "prob.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_p7jtESCTlr",
        "outputId": "77a62dbb-4618-4266-ae10-8c1e5bc34a2c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrWRuu7iCh_3",
        "outputId": "db20000e-5b4c-4073-eee9-f643d5fe9128"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob[torch.arange(32), Y]\n",
        "\n",
        "#This is advanced indexing where we are getting the probabilities of the numbers (in y like 5,12,11,6) from all the 27 probabilities\n",
        "#that prob has"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ffe__IJDNjo",
        "outputId": "fc56843e-7c67-4011-a7dc-3894716b2f6f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8.8819e-14, 5.9384e-12, 3.3697e-08, 3.4759e-08, 3.6165e-05, 1.5945e-01,\n",
              "        6.6612e-07, 7.3043e-10, 4.8271e-05, 9.2898e-14, 2.1269e-10, 3.3449e-05,\n",
              "        1.8259e-09, 9.9476e-08, 1.8675e-09, 7.2076e-10, 1.5526e-14, 4.8154e-17,\n",
              "        6.0614e-09, 3.0683e-05, 3.0906e-03, 1.1244e-07, 1.5560e-06, 1.1734e-06,\n",
              "        5.8804e-08, 7.5055e-16, 5.0728e-01, 9.7256e-08, 7.3120e-11, 2.0486e-01,\n",
              "        7.0513e-03, 1.1462e-05])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating negative loss likelikhood\n",
        "\n",
        "loss = -prob[torch.arange(32), Y].log().mean()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KfnRW9YDsFU",
        "outputId": "23488626-4d39-404c-f177-b712467e6e75"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.0177)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To calculate the negative loss likelihood, we have a direct function\n",
        "#exp of a high +ve number is NaN\n",
        "#Internally pytorch takes the highest number and subtracts it from the input so exp of a high number could be calculated\n",
        "\n",
        "F.cross_entropy(logits, Y)"
      ],
      "metadata": {
        "id": "JW9_g-gDFiWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d300dc03-3fc6-4439-c342-076023967897"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.0177)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#<------------------RESPECTABLE CODE HERE ------------------------------>#\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((27, 2), generator=g)\n",
        "W1 = torch.randn((6, 100), generator=g)\n",
        "b1 = torch.randn(100, generator=g)\n",
        "W2 = torch.randn((100, 27), generator=g)\n",
        "b2 = torch.randn(27, generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "o9V2UaWaZxez"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.nelement() for p in parameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zre85oSlcGVL",
        "outputId": "cc50eaec-6de0-44dd-dc6b-05677ec1ee61"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3481"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "_VBpXY41egc1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Forward pass\n",
        "for _ in range(1000):\n",
        "  emb = C[X] # (32, 3, 2)\n",
        "  h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, Y)\n",
        "  print(loss.item())\n",
        "  #Backward pass\n",
        "\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  #update\n",
        "  for p in parameters:\n",
        "    p.data += -0.1 * p.grad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "Jp6A5g0wdEUY",
        "outputId": "9a64432b-bac6-4eea-d13b-2d849d6622a4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19.505226135253906\n",
            "17.08449363708496\n",
            "15.776531219482422\n",
            "14.833340644836426\n",
            "14.002603530883789\n",
            "13.253260612487793\n",
            "12.57991886138916\n",
            "11.983101844787598\n",
            "11.47049331665039\n",
            "11.051856994628906\n",
            "10.709586143493652\n",
            "10.407632827758789\n",
            "10.127808570861816\n",
            "9.864365577697754\n",
            "9.61450481414795\n",
            "9.376440048217773\n",
            "9.148944854736328\n",
            "8.931111335754395\n",
            "8.722230911254883\n",
            "8.521749496459961\n",
            "8.329227447509766\n",
            "8.144326210021973\n",
            "7.966792106628418\n",
            "7.796451091766357\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3629535438.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (32, 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3461\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3462\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3463\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zJPO9degQy2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}